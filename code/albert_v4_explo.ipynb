{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc30878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sqlite3\n",
    "import pathlib\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Literal, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Pydantic for structured data\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a229011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OMDB_API_KEY = os.getenv(\"OMDB_API_KEY\")\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = pathlib.Path.cwd()\n",
    "if PROJECT_ROOT.name == \"code\":\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "SCRIPT_DIR = str(PROJECT_ROOT / \"code\")\n",
    "DB_FOLDER_PATH = str(PROJECT_ROOT / \"data\" / \"databases\")\n",
    "CHROMA_PATH = str(PROJECT_ROOT / \"data\" / \"chroma_db\")\n",
    "MEMORY_PATH = str(PROJECT_ROOT / \"data\" / \"memory\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e6b74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENUMS AND STRUCTURED DATA MODELS\n",
    "# ============================================================================\n",
    "\n",
    "class QueryIntent(str, Enum):\n",
    "    \"\"\"Query intent types\"\"\"\n",
    "    SQL_QUERY = \"sql_query\"\n",
    "    SEMANTIC_SEARCH = \"semantic_search\"\n",
    "    WEB_SEARCH = \"web_search\"\n",
    "    CLARIFICATION_NEEDED = \"clarification_needed\"\n",
    "    GENERAL_CHAT = \"general_chat\"\n",
    "\n",
    "class DataSource(str, Enum):\n",
    "    \"\"\"Available data sources\"\"\"\n",
    "    SQL_DATABASE = \"sql_database\"\n",
    "    VECTOR_STORE = \"vector_store\"\n",
    "    OMDB_API = \"omdb_api\"\n",
    "    WEB_SEARCH = \"web_search\"\n",
    "\n",
    "class QueryContext(BaseModel):\n",
    "    \"\"\"Rich context for each query\"\"\"\n",
    "    query: str = Field(..., description=\"User's query\")\n",
    "    intent: QueryIntent = Field(..., description=\"Detected intent\")\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    filters: Optional[Dict[str, Any]] = Field(default=None, description=\"Structured filters\")\n",
    "    semantic_keywords: List[str] = Field(default_factory=list)\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    \"\"\"Structured search result\"\"\"\n",
    "    source: DataSource\n",
    "    data: Dict[str, Any]\n",
    "    confidence: float\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    \"\"\"Complete agent state with Pydantic validation\"\"\"\n",
    "    # Session management\n",
    "    session_id: str\n",
    "    thread_id: str\n",
    "\n",
    "    # Query context\n",
    "    original_query: str\n",
    "    query_context: Optional[QueryContext] = None\n",
    "\n",
    "    # Conversation history (managed separately)\n",
    "    chat_history: List[Dict[str, str]] = Field(default_factory=list)\n",
    "\n",
    "    # User profile (loaded from JSON)\n",
    "    user_profile: Dict[str, Any] = Field(default_factory=dict)\n",
    "\n",
    "    # Search results\n",
    "    sql_results: List[SearchResult] = Field(default_factory=list)\n",
    "    semantic_results: List[SearchResult] = Field(default_factory=list)\n",
    "    web_results: List[SearchResult] = Field(default_factory=list)\n",
    "\n",
    "    # Clarification handling\n",
    "    needs_clarification: bool = False\n",
    "    clarification_context: Optional[str] = None\n",
    "    clarification_history: List[Dict[str, str]] = Field(default_factory=list)\n",
    "\n",
    "    # Final output\n",
    "    final_answer: Optional[str] = None\n",
    "    sources_used: List[str] = Field(default_factory=list)\n",
    "    confidence_score: float = 0.0\n",
    "\n",
    "    # Metadata\n",
    "    current_step: str = \"initialized\"\n",
    "    errors: List[str] = Field(default_factory=list)\n",
    "\n",
    "    model_config = {\"arbitrary_types_allowed\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6cff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAFETY GUARDRAILS\n",
    "# ============================================================================\n",
    "\n",
    "# Patterns for pre-guard validation\n",
    "DISALLOWED_PATTERNS = re.compile(\n",
    "    r\"(?i)(credit card|ssn|social security|password|api[_\\s]?key|\"\n",
    "    r\"violent threat|malicious code|sql injection)\"\n",
    ")\n",
    "\n",
    "INJECTION_PATTERNS = re.compile(\n",
    "    r\"(?i)(ignore previous|disregard instructions|system prompt|\"\n",
    "    r\"you are now|forget everything|override)\"\n",
    ")\n",
    "\n",
    "class SafetyGuard:\n",
    "    \"\"\"Production-ready safety system\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def pre_guard(user_input: str) -> tuple[bool, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Validate input before processing.\n",
    "        Returns: (is_safe, error_message)\n",
    "        \"\"\"\n",
    "        # Check for disallowed content\n",
    "        if DISALLOWED_PATTERNS.search(user_input):\n",
    "            return False, \"Input contains prohibited content\"\n",
    "\n",
    "        # Check for prompt injection\n",
    "        if INJECTION_PATTERNS.search(user_input):\n",
    "            return False, \"Input appears to be a prompt injection attempt\"\n",
    "\n",
    "        # Check length\n",
    "        if len(user_input) > 2000:\n",
    "            return False, \"Input exceeds maximum length (2000 characters)\"\n",
    "\n",
    "        return True, None\n",
    "\n",
    "    @staticmethod\n",
    "    def post_guard(output: str, max_length: int = 2000) -> str:\n",
    "        \"\"\"Sanitize output before returning to user\"\"\"\n",
    "        # Truncate if too long\n",
    "        if len(output) > max_length:\n",
    "            output = output[:max_length] + \"... [truncated]\"\n",
    "\n",
    "        # Remove any leaked system prompts\n",
    "        output = re.sub(r\"(?i)(system prompt:|internal note:).*\", \"\", output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_sql_query(query: str) -> tuple[bool, Optional[str]]:\n",
    "        \"\"\"Validate SQL query for safety\"\"\"\n",
    "        dangerous = [\"DROP\", \"DELETE\", \"TRUNCATE\", \"ALTER\", \"CREATE\", \"INSERT\", \"UPDATE\"]\n",
    "        query_upper = query.upper()\n",
    "\n",
    "        for keyword in dangerous:\n",
    "            if keyword in query_upper:\n",
    "                return False, f\"SQL query contains dangerous keyword: {keyword}\"\n",
    "\n",
    "        return True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc2d4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MEMORY MANAGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "class ConversationMemory:\n",
    "    \"\"\"Manages short-term conversation history with JSON persistence\"\"\"\n",
    "\n",
    "    def __init__(self, storage_path: str = MEMORY_PATH):\n",
    "        self.storage_path = pathlib.Path(storage_path) / \"conversations\"\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.in_memory_store = {}  # For InMemoryChatMessageHistory\n",
    "\n",
    "    def get_history(self, session_id: str) -> InMemoryChatMessageHistory:\n",
    "        \"\"\"Get or create history for session\"\"\"\n",
    "        if session_id not in self.in_memory_store:\n",
    "            self.in_memory_store[session_id] = InMemoryChatMessageHistory()\n",
    "            # Load from disk if exists\n",
    "            self._load_from_disk(session_id)\n",
    "        return self.in_memory_store[session_id]\n",
    "\n",
    "    def add_turn(self, session_id: str, user_msg: str, ai_msg: str):\n",
    "        \"\"\"Add conversation turn and save to disk\"\"\"\n",
    "        history = self.get_history(session_id)\n",
    "        history.add_user_message(user_msg)\n",
    "        history.add_ai_message(ai_msg)\n",
    "        # Save to disk\n",
    "        self._save_to_disk(session_id)\n",
    "\n",
    "    def get_recent_context(self, session_id: str, n: int = 5) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get last N conversation turns\"\"\"\n",
    "        history = self.get_history(session_id)\n",
    "        messages = history.messages[-n*2:] if len(history.messages) > n*2 else history.messages\n",
    "        return [{\"role\": m.type, \"content\": m.content} for m in messages]\n",
    "\n",
    "    def _save_to_disk(self, session_id: str):\n",
    "        \"\"\"Save conversation history to JSON\"\"\"\n",
    "        file_path = self.storage_path / f\"{session_id}.json\"\n",
    "        history = self.get_history(session_id)\n",
    "\n",
    "        conversations = [\n",
    "            {\n",
    "                \"role\": msg.type,\n",
    "                \"content\": msg.content,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            for msg in history.messages\n",
    "        ]\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                \"session_id\": session_id,\n",
    "                \"last_updated\": datetime.now().isoformat(),\n",
    "                \"messages\": conversations\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def _load_from_disk(self, session_id: str):\n",
    "        \"\"\"Load conversation history from JSON\"\"\"\n",
    "        file_path = self.storage_path / f\"{session_id}.json\"\n",
    "        if not file_path.exists():\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            history = self.in_memory_store[session_id]\n",
    "            for msg in data.get(\"messages\", []):\n",
    "                if msg[\"role\"] == \"human\":\n",
    "                    history.add_user_message(msg[\"content\"])\n",
    "                elif msg[\"role\"] == \"ai\":\n",
    "                    history.add_ai_message(msg[\"content\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading conversation history: {e}\")\n",
    "\n",
    "    def clear_session(self, session_id: str):\n",
    "        \"\"\"Clear conversation history for session\"\"\"\n",
    "        if session_id in self.in_memory_store:\n",
    "            del self.in_memory_store[session_id]\n",
    "\n",
    "        file_path = self.storage_path / f\"{session_id}.json\"\n",
    "        if file_path.exists():\n",
    "            file_path.unlink()\n",
    "\n",
    "class UserProfileMemory:\n",
    "    \"\"\"Manages long-term user preferences and profiles with JSON persistence\"\"\"\n",
    "\n",
    "    def __init__(self, storage_path: str = MEMORY_PATH):\n",
    "        self.storage_path = pathlib.Path(storage_path) / \"user_profiles\"\n",
    "        self.storage_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def load_profile(self, user_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"Load user profile from disk\"\"\"\n",
    "        profile_file = self.storage_path / f\"{user_id}.json\"\n",
    "        if profile_file.exists():\n",
    "            try:\n",
    "                with open(profile_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading user profile: {e}\")\n",
    "\n",
    "        # Default profile\n",
    "        return {\n",
    "            \"user_id\": user_id,\n",
    "            \"created_at\": datetime.now().isoformat(),\n",
    "            \"preferences\": {\n",
    "                \"favorite_genres\": [],\n",
    "                \"favorite_platforms\": [],\n",
    "                \"preferred_content_type\": None  # \"Movie\" or \"TV Show\"\n",
    "            },\n",
    "            \"search_history\": [],\n",
    "            \"interaction_count\": 0,\n",
    "            \"last_active\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "    def save_profile(self, user_id: str, profile: Dict[str, Any]):\n",
    "        \"\"\"Save user profile to disk\"\"\"\n",
    "        profile_file = self.storage_path / f\"{user_id}.json\"\n",
    "        profile[\"last_active\"] = datetime.now().isoformat()\n",
    "\n",
    "        with open(profile_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(profile, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    def update_preferences(self, user_id: str, query: str, results: List[SearchResult]):\n",
    "        \"\"\"Update profile based on interaction\"\"\"\n",
    "        profile = self.load_profile(user_id)\n",
    "        profile[\"interaction_count\"] += 1\n",
    "\n",
    "        # Add to search history (keep last 50)\n",
    "        profile[\"search_history\"].append({\n",
    "            \"query\": query,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"result_count\": len(results)\n",
    "        })\n",
    "        profile[\"search_history\"] = profile[\"search_history\"][-50:]\n",
    "\n",
    "        # Extract preferences from results\n",
    "        for result in results:\n",
    "            if result.source == DataSource.SQL_DATABASE or result.source == DataSource.VECTOR_STORE:\n",
    "                data = result.data.get(\"results\", [])\n",
    "                if isinstance(data, list):\n",
    "                    for item in data:\n",
    "                        # Extract genres\n",
    "                        if \"genres\" in item or \"listed_in\" in item:\n",
    "                            genres = item.get(\"genres\") or item.get(\"listed_in\", \"\")\n",
    "                            if genres:\n",
    "                                genre_list = [g.strip() for g in genres.split(\",\")]\n",
    "                                for genre in genre_list:\n",
    "                                    if genre and genre not in profile[\"preferences\"][\"favorite_genres\"]:\n",
    "                                        profile[\"preferences\"][\"favorite_genres\"].append(genre)\n",
    "\n",
    "                        # Extract platforms\n",
    "                        if \"platform\" in item:\n",
    "                            platform = item[\"platform\"]\n",
    "                            if platform and platform not in profile[\"preferences\"][\"favorite_platforms\"]:\n",
    "                                profile[\"preferences\"][\"favorite_platforms\"].append(platform)\n",
    "\n",
    "        # Limit lists\n",
    "        profile[\"preferences\"][\"favorite_genres\"] = profile[\"preferences\"][\"favorite_genres\"][-10:]\n",
    "        profile[\"preferences\"][\"favorite_platforms\"] = profile[\"preferences\"][\"favorite_platforms\"][-5:]\n",
    "\n",
    "        self.save_profile(user_id, profile)\n",
    "\n",
    "    def get_personalization_context(self, user_id: str) -> str:\n",
    "        \"\"\"Get formatted context for personalization\"\"\"\n",
    "        profile = self.load_profile(user_id)\n",
    "\n",
    "        if profile[\"interaction_count\"] == 0:\n",
    "            return \"\"\n",
    "\n",
    "        context_parts = []\n",
    "        prefs = profile[\"preferences\"]\n",
    "\n",
    "        if prefs[\"favorite_genres\"]:\n",
    "            context_parts.append(f\"User frequently searches for: {', '.join(prefs['favorite_genres'][:5])}\")\n",
    "        if prefs[\"favorite_platforms\"]:\n",
    "            context_parts.append(f\"Preferred platforms: {', '.join(prefs['favorite_platforms'])}\")\n",
    "        if prefs[\"preferred_content_type\"]:\n",
    "            context_parts.append(f\"Prefers: {prefs['preferred_content_type']}\")\n",
    "\n",
    "        return \"\\n\".join(context_parts) if context_parts else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0bdb7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATABASE UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def build_db_catalog(folder_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Build complete database catalog with paths and schemas\"\"\"\n",
    "    catalog = {\n",
    "        \"folder_path\": folder_path,\n",
    "        \"databases\": {},\n",
    "        \"error\": None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        db_files = [f for f in os.listdir(folder_path)\n",
    "                   if f.endswith(('.db', '.sqlite', '.sqlite3'))]\n",
    "    except FileNotFoundError:\n",
    "        catalog[\"error\"] = f\"Folder {folder_path} not found\"\n",
    "        return catalog\n",
    "\n",
    "    if not db_files:\n",
    "        catalog[\"error\"] = \"No SQLite databases found\"\n",
    "        return catalog\n",
    "\n",
    "    for db_file in db_files:\n",
    "        db_path_full = os.path.join(folder_path, db_file)\n",
    "        db_name = os.path.splitext(db_file)[0]\n",
    "\n",
    "        try:\n",
    "            conn = sqlite3.connect(db_path_full)\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            db_info = {\n",
    "                \"file_name\": db_file,\n",
    "                \"full_path\": db_path_full,\n",
    "                \"tables\": {}\n",
    "            }\n",
    "\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            tables = cursor.fetchall()\n",
    "\n",
    "            for (table_name,) in tables:\n",
    "                cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "                columns_info = cursor.fetchall()\n",
    "\n",
    "                db_info[\"tables\"][table_name] = {\n",
    "                    \"columns\": [\n",
    "                        {\n",
    "                            \"name\": col[1],\n",
    "                            \"type\": col[2],\n",
    "                            \"not_null\": bool(col[3]),\n",
    "                            \"primary_key\": bool(col[5])\n",
    "                        } for col in columns_info\n",
    "                    ],\n",
    "                    \"column_names\": [col[1] for col in columns_info]\n",
    "                }\n",
    "\n",
    "            catalog[\"databases\"][db_name] = db_info\n",
    "            conn.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            catalog[\"databases\"][db_name] = {\n",
    "                \"file_name\": db_file,\n",
    "                \"full_path\": db_path_full,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    return catalog\n",
    "\n",
    "def format_catalog_for_llm(catalog: Dict[str, Any]) -> str:\n",
    "    \"\"\"Format catalog for LLM\"\"\"\n",
    "    if catalog.get(\"error\"):\n",
    "        return f\"ERROR: {catalog['error']}\"\n",
    "\n",
    "    formatted = \" Available Databases:\\n\\n\"\n",
    "\n",
    "    for db_name, db_info in catalog[\"databases\"].items():\n",
    "        if \"error\" in db_info:\n",
    "            formatted += f\" {db_name}: {db_info['error']}\\n\"\n",
    "            continue\n",
    "\n",
    "        formatted += f\"**Database: {db_name}** (file: {db_info['file_name']})\\n\"\n",
    "\n",
    "        for table_name, table_info in db_info[\"tables\"].items():\n",
    "            cols = \", \".join([f\"{col['name']} ({col['type']})\"\n",
    "                            for col in table_info[\"columns\"][:5]])  # Limit for brevity\n",
    "            formatted += f\"  â€¢ Table `{table_name}`: {cols}\\n\"\n",
    "\n",
    "        formatted += \"\\n\"\n",
    "\n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a41dbc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# INTENT CLASSIFICATION AGENT\n",
    "# ============================================================================\n",
    "\n",
    "class IntentClassification(BaseModel):\n",
    "    \"\"\"Structured intent classification\"\"\"\n",
    "    intent: QueryIntent\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    reasoning: str\n",
    "    requires_clarification: bool\n",
    "    clarification_question: Optional[str] = None\n",
    "    extracted_filters: Optional[Dict[str, Any]] = None\n",
    "\n",
    "class IntentClassifierAgent:\n",
    "    \"\"\"Classifies user intent with structured output\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm.with_structured_output(IntentClassification)\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert intent classifier for a movie/TV show database assistant.\n",
    "\n",
    "Available databases:\n",
    "- Netflix (~8,800 titles)\n",
    "- Amazon Prime (~9,600 titles)\n",
    "- Disney+ (~1,400 titles)\n",
    "\n",
    "Classify user queries into these intents:\n",
    "\n",
    "1. SQL_QUERY: Structured queries (year, platform, genre filters)\n",
    "   Examples: \"movies from 2020\", \"Netflix shows\", \"PG-rated films\"\n",
    "\n",
    "2. SEMANTIC_SEARCH: Mood/style/similarity queries\n",
    "   Examples: \"dark thrillers\", \"movies like Inception\", \"heartwarming films\"\n",
    "\n",
    "3. WEB_SEARCH: Current events, recent releases\n",
    "   Examples: \"latest Netflix releases\", \"trending movies today\"\n",
    "\n",
    "4. CLARIFICATION_NEEDED: Ambiguous queries\n",
    "   Examples: \"a movie\", \"good show\", \"something to watch\"\n",
    "\n",
    "5. GENERAL_CHAT: Greetings, off-topic\n",
    "   Examples: \"hello\", \"how are you\", \"thank you\"\n",
    "\n",
    "Extract structured filters when possible:\n",
    "- platform: netflix, amazon_prime, disney_plus\n",
    "- release_year: integer\n",
    "- type: Movie, TV Show\n",
    "- rating: PG, PG-13, R, TV-MA, etc.\n",
    "- genre: Action, Drama, Comedy, etc.\n",
    "\"\"\"),\n",
    "            (\"user\", \"{query}\")\n",
    "        ])\n",
    "        self.chain = self.prompt | self.llm\n",
    "\n",
    "    def classify(self, query: str) -> IntentClassification:\n",
    "        \"\"\"Classify query and return structured intent\"\"\"\n",
    "        return self.chain.invoke({\"query\": query})\n",
    "\n",
    "# ============================================================================\n",
    "# SQL AGENT\n",
    "# ============================================================================\n",
    "\n",
    "class SQLQueryResult(BaseModel):\n",
    "    \"\"\"Structured SQL query result\"\"\"\n",
    "    can_answer_with_sql: bool\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    database: Optional[str] = None\n",
    "    query: Optional[str] = None\n",
    "    expected_columns: List[str] = Field(default_factory=list)\n",
    "    filters_applied: Dict[str, Any] = Field(default_factory=dict)\n",
    "    reasoning: str\n",
    "\n",
    "class SQLAgent:\n",
    "    \"\"\"SQL query generation and execution with safety\"\"\"\n",
    "\n",
    "    def __init__(self, llm, db_catalog: Dict[str, Any]):\n",
    "        self.llm = llm.with_structured_output(SQLQueryResult)\n",
    "        self.db_catalog = db_catalog\n",
    "        self.guard = SafetyGuard()\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are an expert SQL query generator for movie/TV databases.\n",
    "\n",
    "{db_catalog}\n",
    "\n",
    "Generate SAFE, READ-ONLY queries:\n",
    "- Only SELECT statements\n",
    "- Always use LIMIT (max 50)\n",
    "- Use LIKE with wildcards for text search: WHERE title LIKE '%keyword%'\n",
    "- Apply filters from user context\n",
    "- Use proper column names from schema\n",
    "\n",
    "Return structured query plan.\"\"\"),\n",
    "            (\"user\", \"Query: {query}\\nUser Context: {context}\")\n",
    "        ])\n",
    "\n",
    "        self.chain = self.prompt | self.llm\n",
    "\n",
    "    def generate_query(self, query_context: QueryContext, user_context: str = \"\") -> SQLQueryResult:\n",
    "        \"\"\"Generate SQL query with structured output\"\"\"\n",
    "        return self.chain.invoke({\n",
    "            \"query\": query_context.query,\n",
    "            \"context\": user_context,\n",
    "            \"db_catalog\": format_catalog_for_llm(self.db_catalog)\n",
    "        })\n",
    "\n",
    "    def execute_query(self, sql_result: SQLQueryResult) -> SearchResult:\n",
    "        \"\"\"Execute SQL query with safety checks\"\"\"\n",
    "        # Validate query\n",
    "        is_safe, error = self.guard.validate_sql_query(sql_result.query)\n",
    "        if not is_safe:\n",
    "            return SearchResult(\n",
    "                source=DataSource.SQL_DATABASE,\n",
    "                data={\"error\": error, \"results\": []},\n",
    "                confidence=0.0\n",
    "            )\n",
    "\n",
    "        # Execute query\n",
    "        try:\n",
    "            db_path = self.db_catalog[\"databases\"][sql_result.database][\"full_path\"]\n",
    "            conn = sqlite3.connect(db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(sql_result.query)\n",
    "            rows = cursor.fetchall()\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            results = [dict(zip(columns, row)) for row in rows]\n",
    "            conn.close()\n",
    "\n",
    "            return SearchResult(\n",
    "                source=DataSource.SQL_DATABASE,\n",
    "                data={\"results\": results, \"count\": len(results)},\n",
    "                confidence=sql_result.confidence,\n",
    "                metadata={\"database\": sql_result.database, \"filters\": sql_result.filters_applied}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SearchResult(\n",
    "                source=DataSource.SQL_DATABASE,\n",
    "                data={\"error\": str(e), \"results\": []},\n",
    "                confidence=0.0\n",
    "            )\n",
    "\n",
    "# ============================================================================\n",
    "# SEMANTIC RAG AGENT\n",
    "# ============================================================================\n",
    "\n",
    "class SemanticRAGAgent:\n",
    "    \"\"\"Semantic search using vector embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, llm, vectorstore_path: str = CHROMA_PATH):\n",
    "        self.llm = llm\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            api_key=OPENAI_API_KEY\n",
    "        )\n",
    "\n",
    "        # Load vector store (create if doesn't exist)\n",
    "        try:\n",
    "            self.vectorstore = Chroma(\n",
    "                persist_directory=vectorstore_path,\n",
    "                embedding_function=self.embeddings,\n",
    "                collection_name=\"movies_shows\"\n",
    "            )\n",
    "            print(f\" Loaded vector store from {vectorstore_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\" Vector store not found: {e}\")\n",
    "            print(\"   Run create_embeddings.ipynb first!\")\n",
    "            self.vectorstore = None\n",
    "\n",
    "    def search(self, query_context: QueryContext, k: int = 10) -> SearchResult:\n",
    "        \"\"\"Perform semantic search\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return SearchResult(\n",
    "                source=DataSource.VECTOR_STORE,\n",
    "                data={\"error\": \"Vector store not initialized\", \"results\": []},\n",
    "                confidence=0.0\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Build metadata filters\n",
    "            filters = {}\n",
    "            if query_context.filters:\n",
    "                if query_context.filters.get(\"platform\"):\n",
    "                    filters[\"platform\"] = query_context.filters[\"platform\"]\n",
    "                if query_context.filters.get(\"release_year\"):\n",
    "                    filters[\"release_year\"] = query_context.filters[\"release_year\"]\n",
    "                if query_context.filters.get(\"type\"):\n",
    "                    filters[\"type\"] = query_context.filters[\"type\"]\n",
    "\n",
    "            # Execute search\n",
    "            if filters:\n",
    "                results = self.vectorstore.similarity_search(\n",
    "                    query_context.query,\n",
    "                    k=k,\n",
    "                    filter=filters\n",
    "                )\n",
    "            else:\n",
    "                results = self.vectorstore.similarity_search(\n",
    "                    query_context.query,\n",
    "                    k=k\n",
    "                )\n",
    "\n",
    "            formatted_results = [\n",
    "                {\n",
    "                    \"title\": doc.metadata.get(\"title\", \"Unknown\"),\n",
    "                    \"platform\": doc.metadata.get(\"platform\", \"Unknown\"),\n",
    "                    \"type\": doc.metadata.get(\"type\", \"Unknown\"),\n",
    "                    \"year\": doc.metadata.get(\"release_year\", 0),\n",
    "                    \"genres\": doc.metadata.get(\"genres\", \"\"),\n",
    "                    \"description\": doc.page_content[:200] if hasattr(doc, 'page_content') else \"\"\n",
    "                }\n",
    "                for doc in results\n",
    "            ]\n",
    "\n",
    "            return SearchResult(\n",
    "                source=DataSource.VECTOR_STORE,\n",
    "                data={\"results\": formatted_results, \"count\": len(formatted_results)},\n",
    "                confidence=0.8,\n",
    "                metadata={\"filters\": filters}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return SearchResult(\n",
    "                source=DataSource.VECTOR_STORE,\n",
    "                data={\"error\": str(e), \"results\": []},\n",
    "                confidence=0.0\n",
    "            )\n",
    "\n",
    "# ============================================================================\n",
    "# SYNTHESIZER AGENT\n",
    "# ============================================================================\n",
    "\n",
    "class FinalAnswer(BaseModel):\n",
    "    \"\"\"Structured final answer\"\"\"\n",
    "    answer: str = Field(..., description=\"Natural language answer\")\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0)\n",
    "    sources: List[str]\n",
    "    recommendations: Optional[List[Dict[str, str]]] = None\n",
    "    missing_info: List[str] = Field(default_factory=list)\n",
    "    follow_up_suggestions: List[str] = Field(default_factory=list)\n",
    "\n",
    "class SynthesizerAgent:\n",
    "    \"\"\"Synthesizes results from multiple sources into coherent answer\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm.with_structured_output(FinalAnswer)\n",
    "        self.guard = SafetyGuard()\n",
    "\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are Albert, a friendly movie/TV show assistant.\n",
    "\n",
    "Synthesize information from multiple sources into a natural, conversational answer.\n",
    "\n",
    "Guidelines:\n",
    "- Be concise but informative (2-4 sentences for simple queries)\n",
    "- Cite sources naturally (e.g., \"According to our database...\")\n",
    "- If data is incomplete, acknowledge it professionally\n",
    "- Suggest related queries when appropriate\n",
    "- Use friendly, conversational tone\n",
    "- Provide specific recommendations when data is available\n",
    "\n",
    "{user_profile}\"\"\"),\n",
    "            (\"user\", \"\"\"Original query: {query}\n",
    "\n",
    "Available data:\n",
    "\n",
    "SQL Results: {sql_results}\n",
    "\n",
    "Semantic Results: {semantic_results}\n",
    "\n",
    "Web Results: {web_results}\n",
    "\n",
    "Create a comprehensive, natural answer.\"\"\")\n",
    "        ])\n",
    "\n",
    "        self.chain = self.prompt | self.llm\n",
    "\n",
    "    def synthesize(\n",
    "        self,\n",
    "        query: str,\n",
    "        sql_results: List[SearchResult],\n",
    "        semantic_results: List[SearchResult],\n",
    "        web_results: List[SearchResult],\n",
    "        user_profile: str = \"\"\n",
    "    ) -> FinalAnswer:\n",
    "        \"\"\"Synthesize all results into final answer\"\"\"\n",
    "        answer = self.chain.invoke({\n",
    "            \"query\": query,\n",
    "            \"sql_results\": json.dumps([r.data for r in sql_results], default=str),\n",
    "            \"semantic_results\": json.dumps([r.data for r in semantic_results], default=str),\n",
    "            \"web_results\": json.dumps([r.data for r in web_results], default=str),\n",
    "            \"user_profile\": user_profile\n",
    "        })\n",
    "\n",
    "        # Apply post-guard\n",
    "        answer.answer = self.guard.post_guard(answer.answer)\n",
    "\n",
    "        return answer\n",
    "\n",
    "# ============================================================================\n",
    "# MASTER ORCHESTRATOR (Albert V4)\n",
    "# ============================================================================\n",
    "\n",
    "class AlbertV4:\n",
    "    \"\"\"Main orchestrator for Albert V4\"\"\"\n",
    "\n",
    "    def __init__(self, db_catalog: Dict[str, Any], vectorstore_path: str = CHROMA_PATH):\n",
    "        self.llm = llm\n",
    "        self.guard = SafetyGuard()\n",
    "        self.db_catalog = db_catalog\n",
    "\n",
    "        # Initialize agents\n",
    "        self.intent_classifier = IntentClassifierAgent(llm)\n",
    "        self.sql_agent = SQLAgent(llm, db_catalog)\n",
    "        self.semantic_agent = SemanticRAGAgent(llm, vectorstore_path)\n",
    "        self.synthesizer = SynthesizerAgent(llm)\n",
    "\n",
    "        # Initialize memory\n",
    "        self.conv_memory = ConversationMemory()\n",
    "        self.user_memory = UserProfileMemory()\n",
    "\n",
    "        # Build graph\n",
    "        self.app = self._build_graph()\n",
    "\n",
    "    def _build_graph(self) -> StateGraph:\n",
    "        \"\"\"Build LangGraph workflow\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "\n",
    "        # Add nodes\n",
    "        workflow.add_node(\"safety_check\", self._safety_check_node)\n",
    "        workflow.add_node(\"classify_intent\", self._classify_intent_node)\n",
    "        workflow.add_node(\"handle_clarification\", self._handle_clarification_node)\n",
    "        workflow.add_node(\"sql_search\", self._sql_search_node)\n",
    "        workflow.add_node(\"semantic_search\", self._semantic_search_node)\n",
    "        workflow.add_node(\"synthesize\", self._synthesize_node)\n",
    "        workflow.add_node(\"post_guard\", self._post_guard_node)\n",
    "\n",
    "        # Define flow\n",
    "        workflow.add_edge(START, \"safety_check\")\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"safety_check\",\n",
    "            self._route_after_safety,\n",
    "            {\"safe\": \"classify_intent\", \"unsafe\": \"post_guard\"}\n",
    "        )\n",
    "\n",
    "        workflow.add_conditional_edges(\n",
    "            \"classify_intent\",\n",
    "            self._route_after_intent,\n",
    "            {\n",
    "                \"sql\": \"sql_search\",\n",
    "                \"semantic\": \"semantic_search\",\n",
    "                \"clarify\": \"handle_clarification\",\n",
    "                \"chat\": \"synthesize\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "        workflow.add_edge(\"sql_search\", \"synthesize\")\n",
    "        workflow.add_edge(\"semantic_search\", \"synthesize\")\n",
    "        workflow.add_edge(\"handle_clarification\", \"post_guard\")\n",
    "        workflow.add_edge(\"synthesize\", \"post_guard\")\n",
    "        workflow.add_edge(\"post_guard\", END)\n",
    "\n",
    "        # Compile with checkpointing\n",
    "        checkpointer = MemorySaver()\n",
    "        return workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "    def _safety_check_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Pre-guard validation\"\"\"\n",
    "        is_safe, error = self.guard.pre_guard(state.original_query)\n",
    "\n",
    "        if not is_safe:\n",
    "            state.final_answer = f\"I'm sorry, but I cannot process this request. {error}\"\n",
    "            state.current_step = \"safety_blocked\"\n",
    "        else:\n",
    "            state.current_step = \"safety_passed\"\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _classify_intent_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Classify user intent\"\"\"\n",
    "        # Get user profile for personalization\n",
    "        user_profile = self.user_memory.get_personalization_context(state.session_id)\n",
    "\n",
    "        # Classify intent\n",
    "        intent_result = self.intent_classifier.classify(state.original_query)\n",
    "\n",
    "        state.query_context = QueryContext(\n",
    "            query=state.original_query,\n",
    "            intent=intent_result.intent,\n",
    "            confidence=intent_result.confidence,\n",
    "            filters=intent_result.extracted_filters,\n",
    "            semantic_keywords=[]\n",
    "        )\n",
    "\n",
    "        state.needs_clarification = intent_result.requires_clarification\n",
    "        state.clarification_context = intent_result.clarification_question\n",
    "        state.current_step = f\"intent_{intent_result.intent.value}\"\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _handle_clarification_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Handle clarification request\"\"\"\n",
    "        if state.clarification_context:\n",
    "            state.final_answer = state.clarification_context\n",
    "            state.current_step = \"awaiting_clarification\"\n",
    "        else:\n",
    "            state.final_answer = \"I need more information. Could you please be more specific?\"\n",
    "            state.current_step = \"clarification_generic\"\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _sql_search_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute SQL search\"\"\"\n",
    "        user_context = self.user_memory.get_personalization_context(state.session_id)\n",
    "\n",
    "        # Generate query\n",
    "        sql_plan = self.sql_agent.generate_query(state.query_context, user_context)\n",
    "\n",
    "        # Execute if valid\n",
    "        if sql_plan.can_answer_with_sql and sql_plan.query:\n",
    "            result = self.sql_agent.execute_query(sql_plan)\n",
    "            state.sql_results.append(result)\n",
    "            if sql_plan.database:\n",
    "                state.sources_used.append(f\"SQL: {sql_plan.database}\")\n",
    "\n",
    "        state.current_step = \"sql_complete\"\n",
    "        return state\n",
    "\n",
    "    def _semantic_search_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute semantic search\"\"\"\n",
    "        result = self.semantic_agent.search(state.query_context)\n",
    "        state.semantic_results.append(result)\n",
    "        state.sources_used.append(\"Semantic Search (Vector DB)\")\n",
    "        state.current_step = \"semantic_complete\"\n",
    "        return state\n",
    "\n",
    "    def _synthesize_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Synthesize final answer\"\"\"\n",
    "        user_profile = self.user_memory.get_personalization_context(state.session_id)\n",
    "\n",
    "        final = self.synthesizer.synthesize(\n",
    "            state.original_query,\n",
    "            state.sql_results,\n",
    "            state.semantic_results,\n",
    "            state.web_results,\n",
    "            user_profile\n",
    "        )\n",
    "\n",
    "        state.final_answer = final.answer\n",
    "        state.confidence_score = final.confidence\n",
    "        state.sources_used.extend(final.sources)\n",
    "        state.current_step = \"synthesized\"\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _post_guard_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Final output validation\"\"\"\n",
    "        if state.final_answer:\n",
    "            state.final_answer = self.guard.post_guard(state.final_answer)\n",
    "\n",
    "        state.current_step = \"complete\"\n",
    "        return state\n",
    "\n",
    "    def _route_after_safety(self, state: AgentState) -> str:\n",
    "        \"\"\"Route based on safety check\"\"\"\n",
    "        return \"safe\" if state.current_step == \"safety_passed\" else \"unsafe\"\n",
    "\n",
    "    def _route_after_intent(self, state: AgentState) -> str:\n",
    "        \"\"\"Route based on intent\"\"\"\n",
    "        if state.needs_clarification:\n",
    "            return \"clarify\"\n",
    "\n",
    "        if not state.query_context:\n",
    "            return \"chat\"\n",
    "\n",
    "        intent = state.query_context.intent\n",
    "        if intent == QueryIntent.SQL_QUERY:\n",
    "            return \"sql\"\n",
    "        elif intent == QueryIntent.SEMANTIC_SEARCH:\n",
    "            return \"semantic\"\n",
    "        elif intent == QueryIntent.WEB_SEARCH:\n",
    "            return \"semantic\"  # Fallback to semantic for now\n",
    "        else:\n",
    "            return \"chat\"\n",
    "\n",
    "    def query(self, user_query: str, session_id: str = \"default\") -> tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Main entry point for queries.\n",
    "        Returns: (answer, metadata)\n",
    "        \"\"\"\n",
    "        # Load user profile\n",
    "        user_profile = self.user_memory.load_profile(session_id)\n",
    "\n",
    "        # Get recent conversation context\n",
    "        recent_context = self.conv_memory.get_recent_context(session_id, n=3)\n",
    "\n",
    "        # Create initial state\n",
    "        initial_state = AgentState(\n",
    "            session_id=session_id,\n",
    "            thread_id=session_id,\n",
    "            original_query=user_query,\n",
    "            user_profile=user_profile,\n",
    "            chat_history=recent_context\n",
    "        )\n",
    "\n",
    "        # Execute workflow\n",
    "        config = {\"configurable\": {\"thread_id\": session_id}}\n",
    "        result = self.app.invoke(initial_state, config)\n",
    "\n",
    "        # Update conversation memory\n",
    "        if result.final_answer:\n",
    "            self.conv_memory.add_turn(session_id, user_query, result.final_answer)\n",
    "\n",
    "        # Update user profile\n",
    "        all_results = result.sql_results + result.semantic_results + result.web_results\n",
    "        self.user_memory.update_preferences(session_id, user_query, all_results)\n",
    "\n",
    "        # Return answer and metadata\n",
    "        metadata = {\n",
    "            \"confidence\": result.confidence_score,\n",
    "            \"sources\": result.sources_used,\n",
    "            \"intent\": result.query_context.intent.value if result.query_context else \"unknown\",\n",
    "            \"clarification_needed\": result.needs_clarification\n",
    "        }\n",
    "\n",
    "        return result.final_answer or \"I couldn't generate a response.\", metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60678f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initializing Albert V4...\n",
      " Loading database catalog...\n",
      " Loaded 3 databases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vincent\\AppData\\Local\\Temp\\ipykernel_50056\\3195830680.py:151: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  self.embeddings = OpenAIEmbeddings(\n",
      "C:\\Users\\Vincent\\AppData\\Local\\Temp\\ipykernel_50056\\3195830680.py:158: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded vector store from c:\\Users\\Vincent\\GitHub\\Vincent-20-100\\Agentic_Systems_Project_Vlamy\\data\\chroma_db\n",
      "\n",
      "============================================================\n",
      " Albert V4 Ready!\n",
      "============================================================\n",
      "\n",
      "Memory Features:\n",
      "- Short-term: Conversation history (saved to JSON)\n",
      "- Long-term: User profiles & preferences (saved to JSON)\n",
      "- Storage: c:\\Users\\Vincent\\GitHub\\Vincent-20-100\\Agentic_Systems_Project_Vlamy\\data\\memory\n",
      "\n",
      "Try queries like:\n",
      "  - \"movies from 2020\"\n",
      "  - \"dark psychological thrillers\"\n",
      "  - \"show me something similar to Inception\"\n",
      "\n",
      "Type 'quit' to exit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\" Initializing Albert V4...\")\n",
    "\n",
    "    # Build database catalog\n",
    "    print(\" Loading database catalog...\")\n",
    "    db_catalog = build_db_catalog(DB_FOLDER_PATH)\n",
    "\n",
    "    if db_catalog.get(\"error\"):\n",
    "        print(f\" Error: {db_catalog['error']}\")\n",
    "        print(f\"   Please check DB_FOLDER_PATH: {DB_FOLDER_PATH}\")\n",
    "        exit(1)\n",
    "\n",
    "    print(f\" Loaded {len(db_catalog['databases'])} databases\")\n",
    "\n",
    "    # Initialize Albert V4\n",
    "    albert = AlbertV4(db_catalog)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" Albert V4 Ready!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nMemory Features:\")\n",
    "    print(\"- Short-term: Conversation history (saved to JSON)\")\n",
    "    print(\"- Long-term: User profiles & preferences (saved to JSON)\")\n",
    "    print(f\"- Storage: {MEMORY_PATH}\")\n",
    "    print(\"\\nTry queries like:\")\n",
    "    print('  - \"movies from 2020\"')\n",
    "    print('  - \"dark psychological thrillers\"')\n",
    "    print('  - \"show me something similar to Inception\"')\n",
    "    print(\"\\nType 'quit' to exit\\n\")\n",
    "\n",
    "    session_id = \"default_session\"\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"You: \").strip()\n",
    "\n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\n Goodbye!\")\n",
    "                break\n",
    "\n",
    "            if not user_input:\n",
    "                continue\n",
    "\n",
    "            # Query Albert\n",
    "            answer, metadata = albert.query(user_input, session_id)\n",
    "\n",
    "            # Display response\n",
    "            print(f\"\\nAlbert: {answer}\")\n",
    "            print(f\"\\n Metadata:\")\n",
    "            print(f\"   Confidence: {metadata['confidence']:.2f}\")\n",
    "            print(f\"   Intent: {metadata['intent']}\")\n",
    "            print(f\"   Sources: {', '.join(metadata['sources']) if metadata['sources'] else 'None'}\")\n",
    "            print()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n Error: {e}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b2b20",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
